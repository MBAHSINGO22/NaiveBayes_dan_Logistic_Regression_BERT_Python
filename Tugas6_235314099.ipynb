{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9185a1b",
   "metadata": {},
   "source": [
    "EUGENIUS KRISWINAR ADI CAHYA/235314099"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b16d822",
   "metadata": {},
   "source": [
    "Implementasi Naive Bayes dalam Python #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a391fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df504d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Latihan\n",
    "spam_docs = [\"Promo besar hari ini diskon 50%!\", \"Gratis hadiah untuk pelanggan setia!\", \"Segera klaim hadiah spesial kamu!\"]\n",
    "not_spam_docs = [\"Halo, bagaimana kabarmu hari ini?\", \"Jangan lupa meeting besok pukul 10.00\", \"Dokumen penting sudah dikirim ke email\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e84882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi\n",
    "def tokenize(text):\n",
    "    return text.lower().replace(\"!\", \"\").replace(\"%\", \"\").split()\n",
    "spam_words = [word for doc in spam_docs for word in tokenize(doc)]\n",
    "not_spam_words = [word for doc in not_spam_docs for word in tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143e7983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung Probabilitas untuk Naive Bayes\n",
    "spam_counts = Counter(spam_words)\n",
    "not_spam_counts = Counter(not_spam_words)\n",
    "\n",
    "# Hitung vocabulary size dan total kata\n",
    "V = len(set(spam_words + not_spam_words))\n",
    "total_spam = len(spam_words)\n",
    "total_not_spam = len(not_spam_words)\n",
    "\n",
    "def get_prob(word, category_counts, total_count):\n",
    "    \"\"\"\n",
    "    Menghitung probabilitas kata dengan Laplace smoothing\n",
    "    Args:\n",
    "        word: kata yang akan dihitung probabilitasnya\n",
    "        category_counts: Counter object berisi frekuensi kata\n",
    "        total_count: total kata dalam kategori\n",
    "    Returns:\n",
    "        float: nilai probabilitas\n",
    "    \"\"\"\n",
    "    return (category_counts.get(word, 0) + 1) / (total_count + V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce4904a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Probability: 2.0929167208772063e-07\n",
      "Not Spam Probability: 3.9245856642232505e-09\n",
      "Prediction: Spam\n"
     ]
    }
   ],
   "source": [
    "# Prediksi Email Baru\n",
    "email = \"Hadiah besar gratis untuk kamu!\"\n",
    "words = tokenize(email)\n",
    "\n",
    "spam_prob = math.prod([get_prob(word, spam_counts, total_spam) for word in words])\n",
    "not_spam_prob = math.prod([get_prob(word, not_spam_counts, total_not_spam) for word in words])\n",
    "\n",
    "print(\"Spam Probability:\", spam_prob)\n",
    "print(\"Not Spam Probability:\", not_spam_prob)\n",
    "print(\"Prediction:\", \"Spam\" if spam_prob > not_spam_prob else \"Not Spam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff63a4",
   "metadata": {},
   "source": [
    "Kode diatas mengimplementasikan algoritma **Naive Bayes** untuk mendeteksi email spam.\n",
    "\n",
    "Langkah-langkah:\n",
    "1. Mengumpulkan data latihan (spam dan not spam).\n",
    "2. Melakukan tokenisasi teks.\n",
    "3. Menghitung frekuensi kata dengan Counter.\n",
    "4. Menggunakan Laplace smoothing untuk menghitung probabilitas kata.\n",
    "5. Menghitung probabilitas total tiap kategori untuk email baru.\n",
    "6. Menentukan kategori berdasarkan probabilitas terbesar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea3d6c9",
   "metadata": {},
   "source": [
    "Implementasi Naive Bayes dalam Python #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3dca179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "texts = [\n",
    "    \"Promo besar hari ini diskon 50%!\",          # spam\n",
    "    \"Gratis hadiah untuk pelanggan setia!\",      # spam \n",
    "    \"Segera klaim hadiah spesial kamu!\",         # spam\n",
    "    \"Halo, bagaimana kabarmu hari ini?\",         # not spam\n",
    "    \"Jangan lupa meeting besok pukul 10.00\",     # not spam\n",
    "    \"Dokumen penting sudah dikirim ke email\"     # not spam\n",
    "]\n",
    "\n",
    "labels = [\"spam\", \"spam\", \"spam\", \"not spam\", \"not spam\", \"not spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2442f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tokenisasi dan Stopword Removal \n",
    "stopwords = [\"ke\"]\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = text.lower().split()\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "processed_texts = [preprocess(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d00fa4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Buat Vocabulary\n",
    "vocab = {}\n",
    "index = 0\n",
    "for text in processed_texts:\n",
    "    for token in text:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = index\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c2ad3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Konversi ke Vektor (Bag of Words)\n",
    "def text_to_vector(tokens, vocab):\n",
    "    vector = [0] * len(vocab)\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            vector[vocab[token]] += 1\n",
    "    return vector\n",
    "\n",
    "X = [text_to_vector(tokens, vocab) for tokens in processed_texts]\n",
    "y = [1 if label == \"not spam\" else 0 for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d86b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 4. Implementasi Naive Bayes\n",
    "class NaiveBayes:\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha  # Laplace smoothing\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = len(X), len(X[0])\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        # Hitung prior probabilities\n",
    "        self.priors = np.zeros(n_classes)\n",
    "        for c in self.classes:\n",
    "            self.priors[c] = (sum(y == c)) / (n_samples)\n",
    "        \n",
    "        # Hitung likelihood\n",
    "        self.likelihoods = np.zeros((n_classes, n_features))\n",
    "        for c in self.classes:\n",
    "            X_c = [X[i] for i in range(n_samples) if y[i] == c]\n",
    "            total_words_c = sum(sum(x) for x in X_c)\n",
    "            for j in range(n_features):\n",
    "                count_j = sum(x[j] for x in X_c)\n",
    "                self.likelihoods[c][j] = (count_j + self.alpha) / (total_words_c + self.alpha * n_features)\n",
    "        print(self.likelihoods)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            posteriors = []\n",
    "            for c in self.classes:\n",
    "                prior = self.priors[c]\n",
    "                likelihood = math.prod(self.likelihoods[c][j] for j, val in enumerate(x) if val > 0)\n",
    "                posteriors.append(prior * likelihood)\n",
    "            predictions.append(self.classes[np.argmax(posteriors)])\n",
    "            print(posteriors)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c8fa63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04347826 0.04347826 0.04347826 0.04347826 0.04347826 0.04347826\n",
      "  0.04347826 0.06521739 0.04347826 0.04347826 0.04347826 0.04347826\n",
      "  0.04347826 0.04347826 0.04347826 0.02173913 0.02173913 0.02173913\n",
      "  0.02173913 0.02173913 0.02173913 0.02173913 0.02173913 0.02173913\n",
      "  0.02173913 0.02173913 0.02173913 0.02173913 0.02173913 0.02173913]\n",
      " [0.02173913 0.02173913 0.04347826 0.02173913 0.02173913 0.02173913\n",
      "  0.02173913 0.02173913 0.02173913 0.02173913 0.02173913 0.02173913\n",
      "  0.02173913 0.02173913 0.02173913 0.04347826 0.04347826 0.04347826\n",
      "  0.04347826 0.04347826 0.04347826 0.04347826 0.04347826 0.04347826\n",
      "  0.04347826 0.04347826 0.04347826 0.04347826 0.04347826 0.04347826]]\n",
      "[np.float64(1.1652579733553663e-07), np.float64(2.4276207778236796e-09)]\n",
      "[np.int64(0)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Contoh Penggunaan\n",
    "nb = NaiveBayes(alpha=1)\n",
    "nb.fit(X, y)\n",
    "test_text = preprocess(\"Hadiah besar gratis untuk kamu!\")\n",
    "test_vector = text_to_vector(test_text, vocab)\n",
    "print(nb.predict([test_vector]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f93c6a",
   "metadata": {},
   "source": [
    "1. **Dataset & Labeling:**  \n",
    "   Mengandung contoh email dengan label “spam” atau “not spam”.\n",
    "\n",
    "2. **Preprocessing:**  \n",
    "   Tokenisasi dan penghapusan stopword dilakukan agar hanya kata penting yang dipertahankan.\n",
    "\n",
    "3. **Bag of Words:**  \n",
    "   Representasi teks dalam bentuk vektor numerik berdasarkan frekuensi kata.\n",
    "\n",
    "4. **Model Naive Bayes:**  \n",
    "   - **Prior:** Probabilitas tiap kelas (spam dan not spam).\n",
    "   - **Likelihood:** Probabilitas setiap kata muncul pada kelas tertentu menggunakan **Laplace smoothing**.\n",
    "   - **Posterior:** Kombinasi prior dan likelihood untuk menentukan kelas paling mungkin.\n",
    "\n",
    "5. **Prediksi:**  \n",
    "   Model menghitung probabilitas masing-masing kelas dan memilih yang paling tinggi sebagai hasil akhir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032726ce",
   "metadata": {},
   "source": [
    "Implementasi SVM dalam Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94148aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preparation\n",
    "import numpy as np\n",
    "\n",
    "texts_train = [\n",
    "    \"Promo besar hari ini diskon 50%!\",          # spam\n",
    "    \"Gratis hadiah untuk pelanggan setia!\",      # spam\n",
    "    \"Segera klaim hadiah spesial kamu!\",         # spam\n",
    "    \"Halo, bagaimana kabarmu hari ini?\",         # not spam\n",
    "    \"Jangan lupa meeting besok pukul 10.00\",     # not spam\n",
    "    \"Dokumen penting sudah dikirim ke email\"     # not spam\n",
    "]\n",
    "\n",
    "labels_train = [\"spam\", \"spam\", \"spam\", \"not spam\", \"not spam\", \"not spam\"]\n",
    "texts_test = [\n",
    "    \"Kamu mendapatkan hadiah besar!\",\n",
    "    \"Besok ada ujian penting!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "507dbdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Text Preprocessing\n",
    "stopwords = [\"ke\", \"di\", \"dari\", \"untuk\", \"pada\"]\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = text.lower().split()\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "processed_train = [preprocess(text) for text in texts_train]\n",
    "processed_test = [preprocess(text) for text in texts_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da584e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary dari training: {'promo': 0, 'besar': 1, 'hari': 2, 'ini': 3, 'diskon': 4, '50%!': 5, 'gratis': 6, 'hadiah': 7, 'pelanggan': 8, 'setia!': 9, 'segera': 10, 'klaim': 11, 'spesial': 12, 'kamu!': 13, 'halo,': 14, 'bagaimana': 15, 'kabarmu': 16, 'ini?': 17, 'jangan': 18, 'lupa': 19, 'meeting': 20, 'besok': 21, 'pukul': 22, '10.00': 23, 'dokumen': 24, 'penting': 25, 'sudah': 26, 'dikirim': 27, 'email': 28}\n"
     ]
    }
   ],
   "source": [
    "# 3. Vocabulary Building\n",
    "vocab_train = {}\n",
    "index = 0\n",
    "for text in processed_train:\n",
    "    for token in text:\n",
    "        if token not in vocab_train:\n",
    "            vocab_train[token] = index\n",
    "            index += 1\n",
    "print(f\"Vocabulary dari training: {vocab_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2e6fb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train (BoW dari training data):\n",
      "[[1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1]]\n",
      "\n",
      "X_test (BoW dari testing data):\n",
      "[[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 4. Feature Extraction (Bag of Words)\n",
    "def text_to_vector(tokens, vocab):\n",
    "    vector = [0] * len(vocab)\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            vector[vocab[token]] += 1\n",
    "    return vector\n",
    "\n",
    "X_train = np.array([text_to_vector(tokens, vocab_train) for tokens in processed_train])\n",
    "X_test = np.array([text_to_vector(tokens, vocab_train) for tokens in processed_test])\n",
    "y = np.array([1 if label == \"not spam\" else -1 for label in labels_train])\n",
    "\n",
    "print(\"\\nX_train (BoW dari training data):\")\n",
    "print(X_train)\n",
    "print(\"\\nX_test (BoW dari testing data):\")\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "593a9283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      " ➜ Update: w = [-1. -1. -1. -1. -1. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.], b = -1\n",
      " ➜ Tidak ada update untuk sampel ke-2\n",
      " ➜ Tidak ada update untuk sampel ke-3\n",
      " ➜ Update: w = [-1. -1.  0. -1. -1. -1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.], b = 0\n",
      " ➜ Update: w = [-1. -1.  0. -1. -1. -1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.], b = 1\n",
      " ➜ Tidak ada update untuk sampel ke-6\n",
      "\n",
      "Epoch 2\n",
      " ➜ Tidak ada update untuk sampel ke-1\n",
      " ➜ Update: w = [-1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0.  0.  0.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.], b = 0\n",
      " ➜ Tidak ada update untuk sampel ke-3\n",
      " ➜ Tidak ada update untuk sampel ke-4\n",
      " ➜ Tidak ada update untuk sampel ke-5\n",
      " ➜ Update: w = [-1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0.  0.  0.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.], b = 1\n",
      "\n",
      "Epoch 3\n",
      " ➜ Tidak ada update untuk sampel ke-1\n",
      " ➜ Tidak ada update untuk sampel ke-2\n",
      " ➜ Update: w = [-1. -1.  0. -1. -1. -1. -1. -2. -1. -1. -1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.], b = 0\n",
      " ➜ Tidak ada update untuk sampel ke-4\n",
      " ➜ Tidak ada update untuk sampel ke-5\n",
      " ➜ Tidak ada update untuk sampel ke-6\n",
      "\n",
      "Epoch 4\n",
      " ➜ Tidak ada update untuk sampel ke-1\n",
      " ➜ Tidak ada update untuk sampel ke-2\n",
      " ➜ Tidak ada update untuk sampel ke-3\n",
      " ➜ Tidak ada update untuk sampel ke-4\n",
      " ➜ Tidak ada update untuk sampel ke-5\n",
      " ➜ Tidak ada update untuk sampel ke-6\n",
      "\n",
      "Epoch 5\n",
      " ➜ Tidak ada update untuk sampel ke-1\n",
      " ➜ Tidak ada update untuk sampel ke-2\n",
      " ➜ Tidak ada update untuk sampel ke-3\n",
      " ➜ Tidak ada update untuk sampel ke-4\n",
      " ➜ Tidak ada update untuk sampel ke-5\n",
      " ➜ Tidak ada update untuk sampel ke-6\n",
      "\n",
      "Model SVM selesai dilatih!\n",
      "Bobot akhir: [-1. -1.  0. -1. -1. -1. -1. -2. -1. -1. -1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Bias akhir: 0\n"
     ]
    }
   ],
   "source": [
    "# 5. SVM Training\n",
    "w = np.zeros(X_train.shape[1])\n",
    "b = 0\n",
    "eta = 1  # Learning rate\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}\")\n",
    "    for i in range(len(X_train)):\n",
    "        if y[i] * (np.dot(w, X_train[i]) + b) < 1:\n",
    "            w = w + eta * y[i] * X_train[i]\n",
    "            b = b + eta * y[i]\n",
    "            print(f\" ➜ Update: w = {w}, b = {b}\")\n",
    "        else:\n",
    "            print(f\" ➜ Tidak ada update untuk sampel ke-{i+1}\")\n",
    "\n",
    "print(\"\\nModel SVM selesai dilatih!\")\n",
    "print(f\"Bobot akhir: {w}\")\n",
    "print(f\"Bias akhir: {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43682573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediksi untuk sampel uji:\n",
      "[-1.  1.]\n",
      " - Sampel 1: Spam\n",
      " - Sampel 2: Not Spam\n"
     ]
    }
   ],
   "source": [
    "# 6. Prediction\n",
    "def predict(X_test):\n",
    "    return np.sign(np.dot(X_test, w) + b)\n",
    "\n",
    "predictions = predict(X_test)\n",
    "print(\"\\nPrediksi untuk sampel uji:\")\n",
    "print(predictions)\n",
    "for i in range(len(predictions)):\n",
    "    label = \"Not Spam\" if predictions[i] == 1 else \"Spam\"\n",
    "    print(f\" - Sampel {i+1}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e92f6",
   "metadata": {},
   "source": [
    "Kode di atas merupakan **implementasi sederhana algoritma Support Vector Machine (SVM)** untuk klasifikasi teks *spam* dan *not spam* menggunakan pendekatan **Bag of Words (BoW)**.\n",
    "\n",
    "### Langkah-langkah:\n",
    "1. **Data Preparation:**  \n",
    "   Membagi data menjadi *training* dan *testing*.\n",
    "\n",
    "2. **Text Preprocessing:**  \n",
    "   Melakukan tokenisasi dan menghapus *stopwords* untuk mendapatkan kata penting.\n",
    "\n",
    "3. **Vocabulary Building:**  \n",
    "   Membuat daftar kata unik dari data latih untuk dijadikan fitur.\n",
    "\n",
    "4. **Feature Extraction (BoW):**  \n",
    "   Mengubah teks menjadi vektor numerik berdasarkan frekuensi kata.\n",
    "\n",
    "5. **SVM Training:**  \n",
    "   Melatih model dengan memperbarui **bobot (w)** dan **bias (b)** secara manual\n",
    "\n",
    "6. **Prediction:**  \n",
    "   Menghitung hasil prediksi \n",
    "\n",
    "### Kesimpulan:\n",
    "Menunjukkan konsep dasar **SVM Linear** dalam mengklasifikasikan teks tanpa menggunakan library apapun atau bisa dikatakan perhitungan manual.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba559d1",
   "metadata": {},
   "source": [
    "Implementasi Random Forest dalam Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "969164a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Dataset\n",
    "data = [\n",
    "    {\n",
    "        \"hadiah\": 1, \"besar\": 1, \"gratis\": 1, \"untukmu\": 1, \"hari\": 1, \"ini\": 1,\n",
    "        \"besok\": 0, \"kita\": 0, \"ada\": 0, \"pertemuan\": 0, \"kampus\": 0, \n",
    "        \"selamat\": 0, \"anda\": 0, \"memenangkan\": 0, \"jangan\": 0, \"lupa\": 0,\n",
    "        \"tugas\": 0, \"harus\": 0, \"dikumpulkan\": 0, \"terpilih\": 0, \"spesial\": 0,\n",
    "        \"label\": 1\n",
    "    },\n",
    "    {\n",
    "        \"hadiah\": 0, \"besar\": 0, \"gratis\": 0, \"untukmu\": 0, \"hari\": 0, \"ini\": 0,\n",
    "        \"besok\": 1, \"kita\": 1, \"ada\": 1, \"pertemuan\": 1, \"kampus\": 1, \n",
    "        \"selamat\": 0, \"anda\": 0, \"memenangkan\": 0, \"jangan\": 0, \"lupa\": 0,\n",
    "        \"tugas\": 0, \"harus\": 0, \"dikumpulkan\": 0, \"terpilih\": 0, \"spesial\": 0,\n",
    "        \"label\": -1\n",
    "    },\n",
    "    {\n",
    "        \"hadiah\": 1, \"besar\": 0, \"gratis\": 1, \"untukmu\": 0, \"hari\": 0, \"ini\": 0,\n",
    "        \"besok\": 0, \"kita\": 0, \"ada\": 0, \"pertemuan\": 0, \"kampus\": 0,\n",
    "        \"selamat\": 0, \"anda\": 1, \"memenangkan\": 1, \"jangan\": 0, \"lupa\": 0,\n",
    "        \"tugas\": 0, \"harus\": 0, \"dikumpulkan\": 0, \"terpilih\": 0, \"spesial\": 0,\n",
    "        \"label\": 1\n",
    "    }\n",
    "]\n",
    "\n",
    "def gini_impurity(data):\n",
    "    total = len(data)\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    spam_count = sum(1 for row in data if row[\"label\"] == 1)\n",
    "    not_spam_count = total - spam_count\n",
    "    p_spam = spam_count / total\n",
    "    p_not_spam = not_spam_count / total\n",
    "    return 1 - (p_spam ** 2 + p_not_spam ** 2)\n",
    "\n",
    "def split_data(data, feature):\n",
    "    left = [row for row in data if row[feature] == 0]\n",
    "    right = [row for row in data if row[feature] == 1]\n",
    "    return left, right\n",
    "\n",
    "def best_split(data):\n",
    "    best_feature = None\n",
    "    best_gini = 1\n",
    "    best_left, best_right = None, None\n",
    "    \n",
    "    for feature in data[0].keys():\n",
    "        if feature == \"label\":\n",
    "            continue\n",
    "        left, right = split_data(data, feature)\n",
    "        gini_left = gini_impurity(left)\n",
    "        gini_right = gini_impurity(right)\n",
    "        gini_split = (len(left) / len(data)) * gini_left + (len(right) / len(data)) * gini_right\n",
    "        \n",
    "        if gini_split < best_gini:\n",
    "            best_gini = gini_split\n",
    "            best_feature = feature\n",
    "            best_left, best_right = left, right\n",
    "            \n",
    "    return best_feature, best_left, best_right\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, depth=2):\n",
    "        self.depth = depth\n",
    "        self.tree = None\n",
    "        \n",
    "    def build_tree(self, data, depth=0):\n",
    "        if len(set(row[\"label\"] for row in data)) == 1 or depth >= self.depth:\n",
    "            return {\"prediction\": max(set(row[\"label\"] for row in data), \n",
    "                                key=[row[\"label\"] for row in data].count)}\n",
    "        \n",
    "        feature, left, right = best_split(data)\n",
    "        if not left or not right:\n",
    "            return {\"prediction\": max(set(row[\"label\"] for row in data), \n",
    "                                key=[row[\"label\"] for row in data].count)}\n",
    "            \n",
    "        return {\n",
    "            \"feature\": feature,\n",
    "            \"left\": self.build_tree(left, depth + 1),\n",
    "            \"right\": self.build_tree(right, depth + 1)\n",
    "        }\n",
    "        \n",
    "    def fit(self, data):\n",
    "        self.tree = self.build_tree(data)\n",
    "        \n",
    "    def predict_one(self, row, node):\n",
    "        if \"prediction\" in node:\n",
    "            return node[\"prediction\"]\n",
    "        if row[node[\"feature\"]] == 0:\n",
    "            return self.predict_one(row, node[\"left\"])\n",
    "        else:\n",
    "            return self.predict_one(row, node[\"right\"])\n",
    "            \n",
    "    def predict(self, data):\n",
    "        return [self.predict_one(row, self.tree) for row in data]\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=3, sample_size=0.8, depth=2):\n",
    "        self.n_trees = n_trees\n",
    "        self.sample_size = sample_size\n",
    "        self.depth = depth\n",
    "        self.trees = []\n",
    "        \n",
    "    def bootstrap_sample(self, data):\n",
    "        return random.sample(data, int(len(data) * self.sample_size))\n",
    "        \n",
    "    def fit(self, data):\n",
    "        for _ in range(self.n_trees):\n",
    "            sample = self.bootstrap_sample(data)\n",
    "            tree = DecisionTree(depth=self.depth)\n",
    "            tree.fit(sample)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def predict_one(self, row):\n",
    "        predictions = [tree.predict_one(row, tree.tree) for tree in self.trees]\n",
    "        return max(set(predictions), key=predictions.count)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        return [self.predict_one(row) for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1514221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Prediction\n",
    "rf = RandomForest(n_trees=3, depth=2)\n",
    "rf.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38d6164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "test_sms = {\n",
    "    \"hadiah\": 1, \"besar\": 0, \"gratis\": 1, \"untukmu\": 0, \"hari\": 0, \"ini\": 0,\n",
    "    \"besok\": 0, \"kita\": 0, \"ada\": 0, \"pertemuan\": 0, \"kampus\": 0, \n",
    "    \"selamat\": 0, \"anda\": 1, \"memenangkan\": 1, \"jangan\": 0, \"lupa\": 0,\n",
    "    \"tugas\": 0, \"harus\": 0, \"dikumpulkan\": 0, \"terpilih\": 0, \"spesial\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff1fa740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediksi: Spam\n"
     ]
    }
   ],
   "source": [
    "# Predict using Random Forest\n",
    "prediction = rf.predict_one(test_sms)\n",
    "print(\"Prediksi:\", \"Spam\" if prediction == 1 else \"Not Spam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e581f56b",
   "metadata": {},
   "source": [
    "### Langkah-langkah:\n",
    "1. **Dataset:**  \n",
    "   Setiap pesan direpresentasikan dalam bentuk biner (0/1) berdasarkan kata yang muncul, dengan label 1 untuk spam dan -1 untuk not spam.\n",
    "\n",
    "2. **Gini Impurity:**  \n",
    "   Mengukur tingkat ketidaksamaan data pada setiap node pohon/tree.  \n",
    "   Semakin kecil nilai Gini, semakin baik pemisahan datanya.\n",
    "\n",
    "3. **Decision Tree:**  \n",
    "   - Setiap pohon dibangun menggunakan subset data.  \n",
    "   - Pohon akan memilih fitur terbaik untuk membagi data berdasarkan **nilai Gini terkecil**.  \n",
    "   - Kedalaman maksimum pohon diatur dengan parameter depth.\n",
    "\n",
    "4. **Random Forest:**  \n",
    "   - Membentuk beberapa *decision tree* .  \n",
    "   - Setiap pohon dilatih dengan data *bootstrap sample* (subset acak dari data asli).  \n",
    "   - Prediksi akhir diperoleh dengan **voting mayoritas** dari semua pohon.\n",
    "\n",
    "5. **Prediksi:**  \n",
    "   Input teks diuji berdasarkan fitur yang dimilikinya. Jika mayoritas pohon memprediksi 1, maka hasilnya *Spam*, jika -1 maka *Not Spam*.\n",
    "\n",
    "### Kesimpulan:\n",
    "Algoritma **Random Forest** bekerja dengan menggabungkan hasil dari banyak *decision tree* untuk meningkatkan akurasi dan mengurangi *overfitting*.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a58571",
   "metadata": {},
   "source": [
    "Implementasi Logistic Regression dalam Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00ca1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset (fitur dan label)\n",
    "X = [\n",
    "    [1, 0],  # Hadiah besar menanti kamu! → Spam\n",
    "    [0, 1],  # Tugas harus dikumpulkan! → Not Spam\n",
    "    [1, 0],  # Gratis hadiah untukmu! → Spam\n",
    "    [0, 1],  # Jangan lupa tugas kuliah! → Not Spam\n",
    "    [1, 0],  # Selamat! Kamu mendapat hadiah! → Spam\n",
    "]\n",
    "y = [1, 0, 1, 0, 1]  # 1 = Spam, 0 = Not Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fb8d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Fungsi sigmoid\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34b4e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi prediksi\n",
    "def predict(X, w, b):\n",
    "    preds = []\n",
    "    for x in X:\n",
    "        z = sum(w[i] * x[i] for i in range(len(x))) + b\n",
    "        preds.append(sigmoid(z))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7dfc8045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi pembaruan bobot dengan Gradient Descent\n",
    "def train_logistic_regression(X, y, lr=0.1, epochs=100):\n",
    "    w = [0.0 for _ in range(len(X[0]))]  # inisialisasi bobot\n",
    "    b = 0.0  # bias\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(X)):\n",
    "            z = sum(w[j] * X[i][j] for j in range(len(X[0]))) + b\n",
    "            pred = sigmoid(z)\n",
    "            error = y[i] - pred\n",
    "            \n",
    "            # update bobot dan bias\n",
    "            for j in range(len(X[0])):\n",
    "                w[j] += lr * error * X[i][j]\n",
    "            b += lr * error\n",
    "            \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfc142b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "weights, bias = train_logistic_regression(X, y, lr=0.1, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c83a3159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilitas Spam: 0.0521\n",
      "Prediksi: Not Spam\n"
     ]
    }
   ],
   "source": [
    "# Prediksi contoh baru\n",
    "test_input = [0, 1]  # Teks: \"Tugas penting menanti kamu\"\n",
    "z = sum(weights[i] * test_input[i] for i in range(len(test_input))) + bias\n",
    "prob = sigmoid(z)\n",
    "print(\"Probabilitas Spam:\", round(prob, 4))\n",
    "print(\"Prediksi:\", \"Spam\" if prob >= 0.5 else \"Not Spam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b5063",
   "metadata": {},
   "source": [
    "### Langkah-langkah:\n",
    "1. **Dataset:**  \n",
    "   Data input (X) berisi dua fitur biner:\n",
    "   - Fitur 1 = ada kata yang mengindikasikan spam (misal “hadiah”, “gratis”)  \n",
    "   - Fitur 2 = ada kata yang mengindikasikan pesan normal (misal “tugas”, “kuliah”)  \n",
    "   Label y menunjukkan hasil klasifikasi: 1 = Spam, 0 = Not Spam.\n",
    "\n",
    "2. **Fungsi Sigmoid:**  \n",
    "   Fungsi sigmoid(z) digunakan untuk mengubah hasil linear (z) menjadi nilai probabilitas antara 0 dan 1.  \n",
    "\n",
    "3. **Training (Gradient Descent):**  \n",
    "   Bobot (w) dan bias (b) diperbarui berdasarkan *error* antara prediksi dan label sebenarnya\n",
    "\n",
    "4. **Prediksi:**  \n",
    "   Setelah model dilatih, data uji baru dievaluasi untuk menghasilkan probabilitas spam.  \n",
    "   Jika probabilitas ≥ 0.5 → *Spam*, jika < 0.5 → *Not Spam*.\n",
    "\n",
    "### Kesimpulan:\n",
    "Algoritma **Logistic Regression** mempelajari hubungan antara fitur dan label menggunakan fungsi sigmoid dan *gradient descent*, menghasilkan model klasifikasi biner seperti deteksi *spam*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0934cbf",
   "metadata": {},
   "source": [
    "Implementasi BERT dengan LR dalam Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9717233a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import math\n",
    "\n",
    "# 1. Load IndoBERT (tanpa classifier head)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "bert = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0586b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Dataset: teks dan label\n",
    "texts = [\n",
    "    \"Hadiah besar menanti kamu!\",     # Spam\n",
    "    \"Tugas harus dikumpulkan!\",       # Not Spam\n",
    "    \"Gratis hadiah untukmu!\",         # Spam\n",
    "    \"Jangan lupa tugas kuliah!\",      # Not Spam\n",
    "    \"Selamat! Kamu mendapat hadiah!\"  # Spam\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d481a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fungsi: Ambil embedding [CLS] dari BERT\n",
    "def get_cls_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert(**inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()  # shape: (768,)\n",
    "    return cls_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecaaefd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# 4. Ambil semua embedding\n",
    "X = [get_cls_embedding(text) for text in texts]\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fb3962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Fungsi Sigmoid\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0e91886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Logistic Regression Manual\n",
    "def train_logistic_regression(X, y, lr=0.01, epochs=10):\n",
    "    w = [0.0] * len(X[0])  # 768 dimensi\n",
    "    b = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(X)):\n",
    "            z = sum(w[j] * X[i][j] for j in range(len(w))) + b\n",
    "            pred = sigmoid(z)\n",
    "            error = y[i] - pred\n",
    "            \n",
    "            for j in range(len(w)):\n",
    "                w[j] += lr * error * X[i][j]\n",
    "            b += lr * error\n",
    "            \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c8bd798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Latih model\n",
    "weights, bias = train_logistic_regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41119d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Prediksi input baru\n",
    "def predict(text):\n",
    "    x = get_cls_embedding(text)\n",
    "    z = sum(weights[i] * x[i] for i in range(len(x))) + bias\n",
    "    prob = sigmoid(z)\n",
    "    return prob, \"Spam\" if prob >= 0.5 else \"Not Spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be690824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilitas Spam: 0.4866 → Prediksi: Not Spam\n"
     ]
    }
   ],
   "source": [
    "# 9. Uji prediksi\n",
    "test_text = \"Tugas penting menanti kamu!\"\n",
    "prob, label = predict(test_text)\n",
    "print(f\"Probabilitas Spam: {prob:.4f} → Prediksi: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a196218",
   "metadata": {},
   "source": [
    "Kode di atas menggabungkan **BERT** dengan **Logistic Regression** untuk klasifikasi teks *spam* dan *not spam*.\n",
    "\n",
    "### Langkah-langkah Utama:\n",
    "\n",
    "1. **Load Model IndoBERT**\n",
    "   - Menggunakan indobenchmark/indobert-base-p1, model BERT khusus bahasa Indonesia.\n",
    "   - AutoTokenizer dan AutoModel digunakan untuk tokenisasi dan ekstraksi embedding.\n",
    "\n",
    "2. **Ambil Embedding (CLS)**\n",
    "   - Untuk setiap teks, diambil vektor representasi (CLS) dari BERT (ukuran 768 dimensi).\n",
    "   - Vektor ini menjadi fitur input untuk model logistic regression.\n",
    "\n",
    "3. **Logistic Regression (Manual)**\n",
    "   - Bobot (w) dan bias (b) diperbarui dengan **gradient descent** berdasarkan selisih antara label aktual dan prediksi.\n",
    "\n",
    "4. **Prediksi**\n",
    "   - Model menghitung probabilitas Spam dari teks uji baru menggunakan hasil embedding dari BERT.\n",
    "   - Jika probabilitas ≥ 0.5 → *Spam*, jika < 0.5 → *Not Spam*.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
